\section{Conclusion}
\label{sec:conclusions}

In this work, we study the consistency of PLMs with regard to their ability to extract knowledge.
We build a high-quality resource named \resource{} that contains 328 high-quality patterns for thirty-eight relations.
Using \resource{}, we measure consistency in multiple PLMs,
including BERT, RoBERTa, and ALBERT, and show that although
the two latter are superior to BERT in other tasks, they
fall short in terms of consistency. However, the consistency
of these models is generally low.
We release \resource{} along with data tuples from T-REx as
a new benchmark to track knowledge consistency of NLP models.
Finally, we propose a new simple method to improve model consistency, by continuing the pretraining with a novel loss. We show this method to be effective and to improve both the consistency of models as well as their ability to extract the correct facts.

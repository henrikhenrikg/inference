
% Jonathan:
% https://www.aclweb.org/anthology/P12-1013.pdf

% https://www.aclweb.org/anthology/P11-1062.pdf

% https://www.aclweb.org/anthology/P10-1124.pdf


% Mohammad Javad Hosseini:
% https://www.aclweb.org/anthology/P19-1468.pdf

% https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00250

% inference
% https://arxiv.org/pdf/1909.07521.pdf
% https://arxiv.org/pdf/2002.05867.pdf
% https://arxiv.org/pdf/2006.06609.pdf
% https://arxiv.org/pdf/2007.00655.pdf

\section{Background}
\label{sec:background}
There has been significant interest in analyzing how well
PLMs \cite{rogers2020primer} perform on
linguistic tasks
\cite{yoav-syntax,hewitt2019structural,tenney2019bert,amnesic_probing},
commonsense \cite{forbes2019neural,
  da2019cracking,zhang2020language} and reasoning
\cite{talmor2019olmpics,
  kassner-etal-2020-pretrained}, usually assessed by
measures of accuracy.
However, accuracy is just one measure of PLM performance \cite{linzen2020can}. It is
equally important that PLMs do not make contradictory
predictions (cf.\ Figure \ref{fig:overview}), a type of error that humans rarely make. 
There has been relatively little research attention devoted to this question, i.e., to analyze if models behave \emph{consistently}.
One example concerns negation:
\citet{Ettinger_2020} and \citet{kassner-schutze-2020-negated}
show
that models tend to generate facts and their negation, a
type of inconsistent behavior.
\newcite{ravichander-etal-2020-systematicity}
propose paired probes for evaluating consistency.
 Our work is
broader in scope, examining the consistency of PLM behavior across a
range of factual knowledge types and investigating how
models can be made to behave more consistently.


Consistency has also been highlighted as a desirable
property in automatically constructed KBs and downstream NLP
tasks. We now briefly review work along these lines.

\textbf{Consistency in knowledge bases (KBs)} has been
studied in theoretical frameworks in the context of the
satisfiability problem and KB construction, and efficient
algorithms for detecting inconsistencies in KBs have been
proposed \cite{hansen2000probabilistic,andersen2001easy}.
Other work aims to quantify the degree to which KBs are
inconsistent and detects inconsistent statements
\cite{Thimm:2009d,muino2011measuring,Thimm:2013}.



\textbf{Consistency in question answering} was studied by
\citet{ribeiro-etal-2019-red} in two tasks: visual question answering \cite{vqa} and reading comprehension \cite{squad}. They automatically generate questions to test the consistency of QA models.
Their findings suggest that most models are not consistent in their predictions. In addition, they use data augmentation to create more robust models.
\citet{alberti2019synthetic} generate new questions
conditioned on context and answer from a labeled dataset
and by filtering answers that do not provide a consistent
result with the original answer. They show that pretraining on these synthetic data improves QA results.
\citet{consistent-qa}  use data augmentation that complements questions with symmetricity and transitivity, as well as a regularizing loss that penalizes inconsistent predictions.

Work on \textbf{consistency in other domains}
includes \citep{du2019consistent} where  prediction
consistency in procedural text is improved. \citet{ribeiro-etal-2020-beyond} use consistency for more robust evaluation. \citet{li-etal-2019-logic} measure and mitigate inconsistency in natural language inference (NLI), and finally, \citet{camburu2020make} propose a method for measuring inconsistencies in NLI explanations \cite{camburu2018snli}.

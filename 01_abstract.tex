We study consistency in Pretrained Language Models (PLMs), with respect to factual knowledge. That is, the ability of PLMs to consistently predict the same knowledge in cloze-style prompts, while being invariant to paraphrases.
For studying these properties, we create a high-quality resource of cloze-style query paraphrases, which we name \resource{}. It contains paraphrases for forty relations, with an overall of @@ paraphrases.
Using \resource{}, we show that all models we experiment with demonstrate poor consistency capabilities, though with high variance between relations.
We further analyze the representational spaces of these models, and analyze them to provide additional evidence of the poor structure, that suggests these models are currently not suitable for representing knowledge in a robust way.
Finally, we propose a method for improving consistency in these models and show promising results.

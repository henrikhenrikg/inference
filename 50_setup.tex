\section{Experimental Setup}
\label{sec:setup}

\subsection{Models}
We experiment with three PLMs: BERT \cite{bert}, RoBERTa \cite{roberta} and ALBERT \cite{albert}, each with two of their two size variants\footnote{For ALBERT we use the smallest and largest versions.}. Additionally, we use the whole-word-masking version of BERT large, which was shown to perform better in multiple tasks \cite{talmor2019olmpics}.

\subsection{Data}

To query PLMs for knowledge graph like triplets
a cloze-style query is generated using a (subject, relation, object) triple from a knowledge graph and a pattern for the relation that contains variables X and Y for subject and object; e.g, ``X was aired in Y''. The subject is substituted for X and a masked token (e.g. `[MASK]') for Y.

We use knowledge graph data from TREx \cite{trex} which contains 34,039 triples across 41 relations.
We use the patterns from our resource \resource{}, described in the previous section.


\subsection{Evaluation}
\label{sec:eval}

%\ye{I think typed querying should be defined properly}
Following prior work \cite{Xiong2020Pretrained, }: for each relation, we create a
candidate set $\mathcal{C}$ and then predict
$\arg\max_{c\in \mathcal{C}}p(c|q)$ where $p(w|q)$ is the probability
that word $w$ gets predicted in query $q$. For most relations,
there is only one valid entity type, e.g., country for the ``captical-of'' relation.
We choose as $\mathcal{C}$ the set of all possible objects for a specific relation from the TREx dataset.
The candidate set could also be obtained from an entity
typing system
(e.g., \cite{yaghoobzadeh-schutze-2016-intrinsic}), but this
is beyond the scope of this paper.


We compute the accuracy of predictions per relation, or graph, as the number of correct predictions of the object in the entailed pattern, dividing by the total amount of predictions. This metric can be seen as a weighted average, when computing a performance score for each edge, and performing a weighted average based on the number of tuples per pattern.
Additionally, we also report aggregated results for specific edge types that involve a specific transformation: \textit{syntactic}, \textit{lexical}, and both, using the same measurement.

\paragraph{Evaluation Types}
We provide two types of evaluations: \textit{clean} and \textit{complex}.
The first type, \textit{clean} only looks at each relation subgraph where the nodes are entailed by the main pattern. Since we also provide some patterns that entail the main pattern (e.g. ``\subj{} was premiered on \obj{}'' entails ``\subj{} was aired on \obj{}'') these patterns are not always factually correct for the KB triplets. Therefore, 
\section{Experimental Setup}
\label{sec:setup}

\subsection{Models}
We experiment with four PLM variants from three PLMs families: BERT \cite{bert}, BERT whole-word-masking, RoBERTa \cite{roberta} and ALBERT \cite{albert}, each with two of their  size variants.\footnote{For ALBERT we use the smallest and largest versions.}
% Additionally, we use the whole-word-masking version of BERT large, which was shown to perform well in multiple tasks \cite{talmor2019olmpics}.

In addition, we also report a majority baseline that always predicts the same most common object for a specific relation. By definition, this baseline is perfectly consistent.

\subsection{Data}


% To probe for consistency of PLMs we use cloze-style queries using a (subject, relation, object) triple from a KB. We then populate the subjects and objects from the KB into patterns for all triplets in $D$ from \resource{} to create the cloze-style queries; e.g, ``\subj{} was aired in \obj{}''. \subj{} is substituted with the subject and \obj{} with a masked token (e.g. `[MASK]'). \nk{not sure if this is a bit confusing as it is saying the same as earlier a bit differently}\yg{I agree. Don't repeat, and refer to prev description.}

We use knowledge graph data from TREx \cite{trex} which contains 34,039 tuples across 40 relations. To make the results comparable across all models, we remove objects that do not fit a single token in all models' vocabularies. Moreover, some relations contain aggregated data of different types, which makes the original LAMA pattern sometimes obsolete as it does not fit the type (e.g. the tuple (`My Family', `sitcom') in the \textit{genre} relation, that mainly contains music-genre entities).\yg{the prev sentence is not clear}\am{I agree} In these cases, we manually filter these entities from our test set.
Overall, after the filtering, we are left with @@ tuples.
% We use the patterns from our resource \resource{}, described in the previous section. \am{use them for, what?}


\subsection{Evaluation}
\label{sec:eval}

%\ye{I think typed querying should be defined properly}
% Following prior work \cite{Xiong2020Pretrained, }: for each relation, we create a
% candidate set $\mathcal{C}$ and then predict
% $\arg\max_{c\in \mathcal{C}}p(c|q)$ where $p(w|q)$ is the probability
% that word $w$ gets predicted in query $q$. For most relations,
% there is only one valid entity type, e.g., country for the ``captical-of'' relation.
% We choose as $\mathcal{C}$ the set of all possible objects for a specific relation from the TREx dataset.
% The candidate set could also be obtained from an entity
% typing system
% (e.g., \cite{yaghoobzadeh-schutze-2016-intrinsic}), but this
% is beyond the scope of this paper.


We compute the accuracy of predictions per relation, or graph, as the number of consistent predictions for every pattern pair for all triples in the KB for a relation, dividing by the total number of predictions.
\[
\sum_j \sum_{k,l} p_k^i(s_j^i,mask) = p_l^i(s_j^i,mask)
\]
\am{at first glance "=" looks like assignment to me}\am{missing denominator?}
Finally, we aggregate the results for all relations and report a final number as the average of all relations. This metric can be viewed as macro average.\am{I don't understand the metric}
% We also report aggregated results for specific edge types that involve a specific transformation: \textit{syntactic}, \textit{lexical}, and both, using the same measurement. \nk{what about det? Also in the statistics table you call it rest}


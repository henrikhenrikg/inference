\section{Experiments and Results}
\label{sec:experiments}


% \input{tables/consistency_results}

\input{tables/extractability_results}

\subsection{Knowledge Extraction Through Different Patterns}


% \subsection{Consistency and Knowledge}

% \yg{wdym by ``to asses the knowledge and ability of the different patterns to extract knowledge''? in particular, what is the first `knowledge'?}
% To better understand the results, and assess the knowledge and ability of the different patterns to extract the correct knowledge of \resource{}, we report additional metrics that help understanding the results. \sr{need a better motivation. e.g. ``In this section, we provide a finer-grained analysis, measuring consistency scores in various settings". But honestly this looks a bit like a checklist of different, unrelated experiments, and may be better pushed into an appendix.}

We begin by assessing the variability of our patterns as
well as the degree to which they extract the correct
entities. These results are summarized in Table
\ref{tab:extractability_results}.

First, we report \emph{Succ-Patt}, the percentage of
patterns that successfully predicted the right object at
least once. A high score  suggests that the patterns are
decent and models use  them to extract the correct
answer. We see that all PLMs achieve an almost perfect score (between 99.7\% - 100\%).
Next, we report
\emph{Succ-Objs},
the percentage of entities that were predicted correctly by at least one of the patterns.
Succ-Objs quantifies the degree to which the knowledge is stored by the models.
We see that some tuples are not predicted correctly by any of our patterns: the scores vary between 43.8\% for albert-base and 62.2 for BERT-large.
Since the average number of patterns is 8.63, i.e., there
are multiple ways to extract the knowledge, we interpret
these results as evidence that the models do not store a large part of the
\resource{} knowledge.
% \nk{This seems to suggest that it is the fault of the pattern that knowledge is not extracted from the model but an unsuccessful prediction across all models of the same tuple rather says that this knowledge is not stored} 
% The average number of factual knowledge that are correctly extracted varies between 45.3\% to 63.0\%, depending on the model. Notably, the BERT based models are consistently better than the rest in extracting the knowledge. 


% \yg{give this part a paragraph title?}
Finally, we measure \emph{Unk-Const}, consistency  for the subset of tuples
for which no pattern predicted the correct answer correctly;
and \emph{Know-Const},
consistency for the subset of tuples for which
at least one of the patterns for a specific
relation predicted the correct answer.
% A higher score on the first metric would suggest that storing the correct answer is related to the consistency of the model and performing consistent predictions across paraphrases.
Overall, the results indicate that when the factual knowledge is successfully extracted, the model is also more consistent.
For instance, for BERT-large, Know-Const  is 62.7\% and Unk-Const is 47.8\%. 
\nk{I would emphasize this even more and say that our graph is able to distinguish pattern dependent guessing and the other is knowledge stored in the model}
% point on a connection between acquiring the correct knowledge, and the ability to extract it robustly. 
% We refer to these metrics as \textsc{Know} and \textsc{Unk}, respectively, that stands for Knowledgeable and Unknowledgeable consistency.
% The additional consistent measurement we report is Const-Objs, which measures the number of tuples that are predicted consistently across all patterns for a particular relation.
% \ye{not sure if this one is important / alternatively maybe use this one instead of the main consistency score?}


\subsection{Consistency \& Knowledge}
\input{tables/consistency_small} In this section, we report
the overall knowledge metric that was used in \citet{lama},
our consistency results, as well as a new metric that
combines both knowledge and consistency metrics.  The
results are summarized in Table
\ref{tab:consistency_results_small}.

We begin with the LAMA results, that is, the acc@1 of a
model in predicting the object, using the original patterns
from \citet{lama}. acc@1 is defined as accuracy of the
top-ranked object within the candidate set of each
relation. Our numbers differ from \citet{lama} as we use a
candidate set (\S\ref{sec:probe}) and only consider KB
triples whose object is a single token
(\S\ref{setupdata}). The results range between 28.4\%
(albert-base) and 45.0\% (bert-large whole-word-masking).

Next, we report consistency measure (\S\ref{sec:eval}).
BERT-based models achieve the highest scores. There is a consistent improvement from the \textit{base} to \textit{large} versions of each model.
% , although it is not always significantly \am{is there a significance test?} higher (e.g. 57.3\% to 59.1\% in BERT, and 50.1\% to 54.8\% in RoBERTa).
In contrast to much previous work that observed quantitative and qualitative improvements of RoBERTa-based models over BERT, in terms of consistency, BERT is more consistent than RoBERTa and ALBERT.
Still, the overall results are remarkably low (59.6\% for
the best model), %\yg{meaning that out of XXX, YYY are
%ZZZ}),
even more remarkably so because the restricted candidate set
makes the task easier.
We note that the results are highly variant between models
(the best performance of any of the models on
`capital of' is 94\% whereas it is 44\% for `owned by')
and relations (models' performance on `original language of film or TV show'  varies between 52\% and 90\%).
% We report the full consistency results for each relation in the Appendix.

Finally, we report
\emph{Group-Score}: it evaluates an object as correct only
if \emph{all} patterns predict the object
correctly. Group-Score is much stricter and it combines  the
requirements
of consistency and factual correctness.
The results are much lower than  LAMA acc@1, as expected,
but follow the same trend: albert-base perform worse (15.8\%) and bert-large  best  (26.8\%).





% For brevity, we report the results solely on BERT-large, which achieved the highest results. The results can be viewed in Table \ref{tab:bert-results}.

A striking result of the model comparison is
is the clear superiority of
BERT, both in knowledge accuracy and knowledge
consistency. This result, which we hypothesise to be due to
the training data, may have a broader impact on models to
come: Training bigger models with more data is not always
beneficial. Since Wikipedia is likely the largest unified source
of factual knowledge that exists in unstructured data,
giving prominence to Wikipedia in pretraining a model makes it more
likely that the model will incorporate the factual knowledge
well. 
This may indicate that  the consistency of models such as GPT-3 \cite{gpt3},
(or other future models) -- i.e., models that
have a much larger size and
are trained on very large corpora of
which Wikipedia is just a small part -- may suffer.

% \am{this is unsubstantiated, you can add "we hypothesize" to make this claim}% and retain more factual knowledge.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5



% \subsection{Consistency}

% The results for the different models are summarized in Table \ref{tab:consistency_main}.
% % First, we report the average consistency results for all graphs, as describe in Section \ref{sec:eval}.
% The last column (``Consistency'') shows the average results across all relations \yg{consider oredering the table columns in the order of their discussion}\am{I actually like it in the end, its easier to find in a glance}



\input{tables/robertas_consistency}

% \enote{hs}{below: i would prefer ``(training) corpus size''
%   over ``number of tokens'' if that's what you mean}

\paragraph{Effect of Pretraining Corpus Size}
Next, we study the question of whether the number of tokens used during the pretraining contributes to consistency.
We use the pretrained RoBERTas model provided by \citet{robertas} and repeat the experiments on four additional models.
These models are RoBERTa-based models, that were trained on a sample of Wikipedia and the book corpus, with varying training size, and parameters. In practice, we use one of the three published models for each configuration and report the average accuracy over the relations for each model in Table \ref{tab:robertas}.
Overall, the consistency and the knowledge scores (LAMA and Group-Score) improve when trained on more data. However, there is an interesting outlier to this trend.
First, the model that was trained on one million tokens is more consistent than the models trained on ten and one-hundred million tokens. A potentially crucial difference is that this model is also smaller in terms of the number of parameters than the rest, in order to avoid overfitting. Still, it is nonetheless interesting that a model that is trained on significantly less data can achieve better consistency. On the other hand, the knowledge scores are lower, arguably due to the model being exposed to less factual knowledge during pretraining.

% Finally, the original RoBERTa that was trained on ten billion parameters, achieves the same consistency performance as the one trained on one billion tokens, suggesting a limit to the benefit of training examples in pretraining. \nk{not sure about this as the table is not there yet, but shouldn't it be: Finally, the original RoBERTa that was trained on ten billion tokens, achieves the same consistency performance as the one trained on one billion tokens, suggesting on a limit to the benefit of large training corpora. }



% \input{tables/entailment_splits}

% \input{tables/entailment_types}


% \begin{figure*}[t!]
% \centering

% \includegraphics[width=1.\textwidth]{figures/results}

% \caption{Results summary, by relation, by split.}
% \label{fig:resuls}
% % \vspace{-6mm}
% \end{figure*}

% \input{tables/bert_results_extended}



\enote{hs}{below: many people might think that a difference
  in tense is a difference in syntax. Tense is part of
  syntax?
}

\subsection{Do PLMs Generalize Over Syntax?} 

\input{tables/syntax_results}

Many papers have found models (especially PLMs) to naturally
encode syntax
\cite{linzen2016assessing,marvin-linzen-2018-targeted,yoav-syntax,hewitt2019structural}.
%How does this reflect in their ability to abstract
%knowledge and produce it while controlling for syntactic
%variations?
Does this mean that PLMs have successfully abstracted
knowledge and can comprehend and produce it regardless of
syntactic variation?
We consider two scenarios. (1) two patterns differ only in
syntax. (2) Both  syntax and  lexical choice are the same.
Here we define identical syntax as identical dependency
paths between subject and object.
We parse all patterns from \resource{} using a dependency parser \cite{spacy}\footnote{\url{https://spacy.io/}} and retain the path between  subject and object.

Success on (1) indicates
that the model's knowledge processing is robust to syntactic
variation.
Success on (2) indicates
that the model's knowledge processing is robust to
variation in word order and tense.

%the ability of the model to abstract the knowledge, and extract it using different syntactic patterns. Success on (2) indicates the abstraction over word order and tense. This way, we can test for consistency in models, while controlling for syntactic variation.\footnote{We consider two patterns to have the same syntax if the path between the entities are equal} 
% \sr{needs a footnote on our \emph{proxy} for syntactic change.}
% staying invariant to alternations while the syntax is the only component that differs, and when 
% Then, for every pattern pair, we split into two groups: (1) patterns where the only difference is the syntactic path, but the lexical items are equal, and (2) patterns where both the syntactic path and the lexical items are identical. \nk{This sentence seems repetitive}
% we filter out all cases where the syntactic path is different. 


Table \ref{tab:syntax_results}
reports results.
While the results are not comparable to the main results on
the entire dataset as the pattern subsets are different, overall
the results are low: 61.5\% for BERT-large when only 
syntax differs, and 78.7\% when syntax is
identical.  This demonstrates that while PLMs have impressive syntactic
abilities, 
they struggle to extract factual knowledge in the face of
tense, word-order and syntactic variation.

% These results are surprising, since the ability to abstract over syntax is perhaps the easier abstraction, and was expected to perform better, given other results on PLMs syntactic abilities. \sr{not sure if we can say it's easier/harder.}

\citet{mccoy2019right}  show that supervised models trained on an NLI
dataset \cite{dagan-rte,snli} such as MNLI \cite{mnli} use
superficial syntactic heuristics rather than more
generalizable properties of the data.
Our results show that pretrained PLMs have  problems along
the same lines:
they are not robust to surface variation.

%These results reminiscent of the findings of
%However, we demonstrate that even PLMs are susceptible to these errors. 

\section{Experiments and Results}
\label{sec:experiments}


% \input{tables/consistency_results}

\input{tables/extractability_results}

\subsection{Knowledge Extraction through Different Patterns}


% \subsection{Consistency and Knowledge}

% \yg{wdym by ``to asses the knowledge and ability of the different patterns to extract knowledge''? in particular, what is the first `knowledge'?}
% To better understand the results, and assess the knowledge and ability of the different patterns to extract the correct knowledge of \resource{}, we report additional metrics that help to understand the results. \sr{need a better motivation. e.g. ``In this section, we provide a finer-grained analysis, measuring consistency scores in various settings". But honestly, this looks a bit like a checklist of different, unrelated experiments, and may be better pushed into an appendix.}

We begin by assessing our patterns as
well as the degree to which they extract the correct
entities. These results are summarized in Table
\ref{tab:extractability_results}.

First, we report \emph{Succ-Patt}, the percentage of
patterns that successfully predicted the right object at
least once. A high score suggests that the patterns are of high quality and enable the models to extract the correct answers. All PLMs achieve a perfect score.
Next, we report
\emph{Succ-Objs},
the percentage of entities that were predicted correctly by at least one of the patterns.
\textit{Succ-Objs} quantifies the degree to which the models
``have'' the required knowledge.
We observe that some tuples are not predicted correctly by any of our patterns: the scores vary between 45.8\% for ALBERT-base and 65.7\% for BERT-large. %\nk{should we say we consider these facts to be \emph{Known} in reference to \emph{Know-Const}.}
With an average number of 8.63 patterns per relation, there are multiple ways to extract the knowledge, we thus interpret these results as evidence that a large part of T-REx knowledge is not stored in these models.
% \nk{should we say we consider these facts to be \emph{Unknown} in reference to \emph{Unk-Const}.}
% \nk{This seems to suggest that it is the fault of the pattern that knowledge is not extracted from the model but an unsuccessful prediction across all models of the same tuple rather says that this knowledge is not stored} 
% The average number of factual knowledge that is correctly extracted varies between 45.3\% to 63.0\%, depending on the model. Notably, the BERT-based models are consistently better than the rest in extracting the knowledge. 


% \yg{give this part a paragraph title?}
Finally, we measure \emph{Unk-Const}, a consistency measure for the subset of tuples
for which no pattern predicted the correct answer; % correctly;
and \emph{Know-Const}, consistency for the subset where
at least one of the patterns for a specific
relation predicted the correct answer.
This split into subsets is
based on \textit{Succ-Objs}.
% A higher score on the first metric would suggest that storing the correct answer is related to the consistency of the model and performing consistent predictions across paraphrases.
Overall, the results indicate that when the factual knowledge is successfully extracted, the model is also more consistent.
For instance, for BERT-large, \textit{Know-Const}  is 65.2\% and \textit{Unk-Const} is 48.1\%. 
% The analysis of knowledge consistency permits the distinction between when the knowledge is robustly stored inside the parameters of a PLM (based on information extracted from training corpora) as compared to when its predictions rely on spurious correlations.

% suggestion:
% This analysis also enables us to separate between facts that are robustly encoded in the model, compared to mere `guesses', which may arise from some heuristic or spurious correlations with certain patterns \cite{poerner2020bert}.

% point on a connection between acquiring the correct knowledge, and the ability to extract it robustly. 
% We refer to these metrics as \textsc{Know} and \textsc{Unk}, respectively, that stands for Knowledgeable and Unknowledgeable consistency.
% The additional consistent measurement we report is Const-Objs, which measures the number of tuples that are predicted consistently across all patterns for a particular relation.
% \ye{not sure if this one is important / alternatively maybe use this one instead of the main consistency score?}


\subsection{Consistency \& Knowledge}
\input{tables/consistency_small} In this section, we report
the overall knowledge measure that was used in \citet{lama} (\textit{Accuracy}),
the consistency measure (\textit{Consistency}),
and \textit{Consistent-\
  Acc}, which combines knowledge and consistency  (\textit{Consistent-Acc}). %\nk{An alternative would be to mention and define all of these in 5.3}
The results are summarized in Table
\ref{tab:consistency_results_small}.

We begin with the \textit{Accuracy} results. 
The results range between 29.8\% (ALBERT-base) and 48.7\% (BERT-large whole-word-masking).
Notice that our numbers differ from \citet{lama} as we use a
candidate set (\S\ref{sec:probe}) and only consider KB
triples whose object is a single token in all the PLMs we consider (\S\ref{setupdata}). 

Next, we report  \textit{Consistency}  (\S\ref{sec:eval}).
The BERT models achieve the highest scores. There is a consistent improvement from  {base} to {large} versions of each model.
% , although it is not always significantly \am{is there a significance test?} higher (e.g. 57.3\% to 59.1\% in BERT, and 50.1\% to 54.8\% in RoBERTa).
In contrast to previous work that observed quantitative and qualitative improvements of RoBERTa-based models over BERT \cite{roberta,talmor2019olmpics}, in terms of consistency, BERT is more consistent than RoBERTa and ALBERT.
Still, the overall results are low (61.1\% for
the best model), %\yg{meaning that out of XXX, YYY are
%ZZZ}),
even more remarkably so because the restricted candidate set
makes the task easier.
We note that the results are highly variant between models
 (performance on \textit{original-language} varies between
52\% and 90\%), and relations (BERT-large performance is
92\% on \textit{capital-of} and 44\% on \textit{owned-by}).
% We report the full consistency results for each relation in the Appendix.

Finally, we report
\emph{Consistent-Acc}:
the results are much lower than for \textit{Accuracy}, as expected,
but follow similar trends: RoBERTa-base performs worse (16.4\%) and BERT-large best  (29.5\%).

Interestingly, we find strong correlations between Accuracy
and Consistency, ranging from 67.3\% for RoBERTa-base to
82.1\% for BERT-large (all with small p-values $\ll 0.01 $).



% For brevity, we report the results solely on BERT-large, which achieved the highest results. The results can be viewed in Table \ref{tab:bert-results}.

A striking result of the model comparison is
the clear superiority of
BERT, both in knowledge accuracy (which was also observed by \citet{autoprompt}) and knowledge
consistency. \yenew{We hypothesize this result is caused by
  the different sources of training data: although Wikipedia
  is part of the training data for all models we consider,
  for BERT it is the main data source, but for RoBERTa and
  ALBERT it is only a small portion. Thus, when using
  additional data, some of the facts may be forgotten, or
  contradicted in the other corpora; this can diminish
  knowledge and compromise consistency behavior.} 
Thus, since Wikipedia is likely the largest unified source of factual knowledge that exists in unstructured data, giving it prominence in pretraining  makes it more likely that the model will incorporate Wikipedia's factual knowledge well.
These results may have a broader impact on models to
come: Training bigger models with more data (such as GPT-3 \cite{gpt3}) is not always beneficial. 
% This may indicate that the consistency of models such as GPT-3 \cite{gpt3}
% or other future models -- i.e., models that
% have a much larger size and
% are trained on very large corpora of
% which Wikipedia is just a small part -- may suffer.

% \am{this is unsubstantiated, you can add "we hypothesize" to make this claim}% and retain more factual knowledge.


\paragraph{Determinism}
We also measure determinism for N-M relations, i.e., we use
the same measure as \textit{Consistency}, but since difference predictions may be factually correct, these do not necessarily convey consistency violations, but indicate non-determinism. For brevity, we do not present all results, but the trend is similar to the consistency result (although not comparable, as the relations are different): 52.9\% and 44.6\% for BERT-large and RoBERTa-base, respectively.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5



% \subsection{Consistency}





\input{tables/robertas_consistency}

% \enote{hs}{below: i would prefer ``(training) corpus size''
%   over ``number of tokens'' if that's what you mean}

\paragraph{Effect of Pretraining Corpus Size}
Next, we study the question of whether the number of tokens used during pretraining contributes to consistency.
We use the pretrained RoBERTa models from \citet{robertas} and repeat the experiments on four additional models.
These are RoBERTa-based models, trained on a sample of Wikipedia and the book corpus, with varying training sizes and parameters. We use one of the three published models for each configuration and report the average accuracy over the relations for each model in Table \ref{tab:robertas}.
Overall, \textit{Accuracy} and
\textit{Consistent-Acc} improve
with more training data.
However, there is an interesting outlier to this trend:
The model that was trained on one million tokens is more consistent than the models trained on ten and one-hundred million tokens. A potentially crucial difference is that this model has many fewer parameters than the rest (to avoid overfitting). It is nonetheless interesting that a model that is trained on significantly less data can achieve better consistency. On the other hand, it's accuracy scores are lower, arguably due to the model being exposed to less factual knowledge during pretraining.


\subsection{Do PLMs Generalize Over Syntactic Configurations?} 

Many papers have found neural models (especially PLMs) to naturally
encode syntax
\cite{linzen2016assessing,belinkov2017neural,marvin-linzen-2018-targeted,belinkov2019analysis,yoav-syntax,hewitt2019structural}.
%How does this reflect in their ability to abstract
%knowledge and produce it while controlling for syntactic
%variations?
Does this mean that PLMs have successfully abstracted
knowledge and can comprehend and produce it regardless of
syntactic variation?
We consider two scenarios. (1) Two patterns differ only in
syntax. (2) Both syntax and lexical choice are the same.
As a proxy, we define syntactic equivalence when the dependency path between the subject and object are identical.
We parse all patterns from \resource{} using a dependency parser \cite{spacy}\footnote{\url{https://spacy.io/}} and retain the path between the entities.
Success on (1) indicates that the model's knowledge processing is robust to syntactic variation. Success on (2) indicates that the model's knowledge processing is robust to
variation in word order and tense.

\input{tables/syntax_results}
%the ability of the model to abstract the knowledge, and extract it using different syntactic patterns. Success on (2) indicates the abstraction over word order and tense. This way, we can test for consistency in models, while controlling for syntactic variation.\footnote{We consider two patterns to have the same syntax if the path between the entities are equal} 
% \sr{needs a footnote on our \emph{proxy} for syntactic change.}
% staying invariant to alternations while the syntax is the only component that differs, and when 
% Then, for every pattern pair, we split into two groups: (1) patterns where the only difference is the syntactic path, but the lexical items are equal, and (2) patterns where both the syntactic path and the lexical items are identical. \nk{This sentence seems repetitive}
% we filter out all cases where the syntactic path is different. 


Table \ref{tab:syntax_results}
reports results.
While the results are not comparable to the main results on
the entire dataset as the pattern subsets are different,
they are higher than the general results: 67.5\% for BERT-large when only 
the syntax differs and 78.7\% when the syntax is
identical. This demonstrates that while PLMs have impressive syntactic abilities, 
they struggle to extract factual knowledge in the face of
tense, word-order, and syntactic variation.

% These results are surprising, since the ability to abstract over syntax is perhaps the easier abstraction, and was expected to perform better, given other results on PLMs syntactic abilities. \sr{not sure if we can say it's easier/harder.}

\citet{mccoy2019right}  show that supervised models trained on MNLI \cite{mnli}, an NLI
dataset \cite{snli}, use
superficial syntactic heuristics rather than more
generalizable properties of the data.
Our results indicate that PLMs have  problems along
the same lines:
they are not robust to surface variation.

%These results reminiscent of the findings of
%However, we demonstrate that even PLMs are susceptible to these errors. 


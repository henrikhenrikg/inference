@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@inproceedings{ijcai2020-537,
  title     = {Transformers as Soft Reasoners over Language},
  author    = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  editor    = {Christian Bessiere},	
  pages     = {3882--3890},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/537},
  url       = {https://doi.org/10.24963/ijcai.2020/537},
}

@inproceedings{Bosselut2019COMETCT,
  title={COMET: Commonsense Transformers for Automatic Knowledge Graph Construction},
  author={Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and Asli Ã‡elikyilmaz and Yejin Choi},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@misc{heinzerling2020language,
      title={Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries}, 
      author={Benjamin Heinzerling and Kentaro Inui},
      year={2020},
      eprint={2008.09036},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{li-etal-2019-logic,
    title = "A Logic-Driven Framework for Consistency of Neural Models",
    author = "Li, Tao  and
      Gupta, Vivek  and
      Mehta, Maitrey  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1405",
    doi = "10.18653/v1/D19-1405",
    pages = "3924--3935",
    abstract = "While neural models show remarkable accuracy on individual predictions, their internal beliefs can be inconsistent across examples. In this paper, we formalize such inconsistency as a generalization of prediction error. We propose a learning framework for constraining models using logic rules to regularize them away from inconsistency. Our framework can leverage both labeled and unlabeled examples and is directly compatible with off-the-shelf learning schemes without model redesign. We instantiate our framework on natural language inference, where experiments show that enforcing invariants stated in logic can help make the predictions of neural models both accurate and consistent.",
}

@inproceedings{kassner-etal-2020-pretrained,
    title = "Are Pretrained Language Models Symbolic Reasoners over Knowledge?",
    author = {Kassner, Nora  and
      Krojer, Benno  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.conll-1.45",
    pages = "552--564",
    abstract = "How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.",
}

@inproceedings{lama,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.",
}


@inproceedings{yanaka-etal-2020-neural,
    title = "Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?",
    author = "Yanaka, Hitomi  and
      Mineshima, Koji  and
      Bekki, Daisuke  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.543",
    doi = "10.18653/v1/2020.acl-main.543",
    pages = "6105--6117",
    abstract = "Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits. A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets. However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set. This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set.",
}

@article{forbes2019neural,
  title={Do Neural Language Representations Learn Physical Commonsense?},
  author={Forbes, Maxwell and Holtzman, Ari and Choi, Yejin},
  journal={arXiv preprint arXiv:1908.02899},
  year={2019}
}

@article{Ettinger_2020,
   title={What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models},
   volume={8},
   ISSN={2307-387X},
   url={http://dx.doi.org/10.1162/tacl_a_00298},
   DOI={10.1162/tacl_a_00298},
   journal={Transactions of the Association for Computational Linguistics},
   publisher={MIT Press - Journals},
   author={Ettinger, Allyson},
   year={2020},
   month={Jan},
   pages={34â€“48}
}

@inproceedings{da2019cracking,
    title = "Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations",
    author = "Da, Jeff  and
      Kasai, Jungo",
    booktitle = "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-6001",
    doi = "10.18653/v1/D19-6001",
    pages = "1--12",
    abstract = "Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT{'}s commonsense representation abilities. First, we probe BERT{'}s ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT{'}s pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.",
}


@article{DBLP:journals/corr/ForbesC17,
  author    = {Maxwell Forbes and
               Yejin Choi},
  title     = {Verb Physics: Relative Physical Knowledge of Actions and Objects},
  journal   = {CoRR},
  volume    = {abs/1706.03799},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03799},
  archivePrefix = {arXiv},
  eprint    = {1706.03799},
  timestamp = {Mon, 13 Aug 2018 16:45:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ForbesC17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{rogers2020primer,
  title={A primer in bertology: What we know about how bert works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2002.12327},
  year={2020}
}

@inproceedings{ravichander-etal-2020-systematicity,
    title = "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}",
    author = "Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Suleman, Kaheer  and
      Trischler, Adam  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.starsem-1.10",
    pages = "88--102",
    abstract = "Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: \textit{do probing studies shed light on systematic knowledge in BERT representations?} As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT {`}understands{'} a concept, and it cannot be expected to systematically generalize across applicable contexts.",
}

@inproceedings{goodwin-etal-2020-probing,
    title = "Probing Linguistic Systematicity",
    author = "Goodwin, Emily  and
      Sinha, Koustuv  and
      O{'}Donnell, Timothy J.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.177",
    doi = "10.18653/v1/2020.acl-main.177",
    pages = "1958--1969",
    abstract = "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",
}

@inproceedings{kassner-schutze-2020-negated,
    title = "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
    author = {Kassner, Nora  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.698",
    doi = "10.18653/v1/2020.acl-main.698",
    pages = "7811--7818",
    abstract = "Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated ({`}{`}Birds cannot [MASK]{''}) and non-negated ({`}{`}Birds can [MASK]{''}) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add {``}misprimes{''} to cloze questions ({`}{`}Talk? Birds can [MASK]{''}). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
}


@inproceedings{schmitt-schutze-2019-sherliic,
    title = "{S}her{LI}i{C}: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference",
    author = {Schmitt, Martin  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1086",
    doi = "10.18653/v1/P19-1086",
    pages = "902--914",
    abstract = "We present SherLIiC, a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) {\textasciitilde}960k unlabeled InfCands, and (ii) {\textasciitilde}190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands. We also show that, due to its construction, many of SherLIiC{'}s correct InfCands are novel and missing from existing rule bases. We evaluate a large number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI). We show that SherLIiC poses a tough challenge to existing NLI systems.",
}


@article{talmor2019olmpics,
author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
title = {oLMpics-On What Language Model Pre-training Captures},
journal = {Transactions of the Association for Computational Linguistics},
volume = {8},
number = {},
pages = {743-758},
year = {2020},
doi = {10.1162/tacl\_a\_00342},
URL = { 
        https://doi.org/10.1162/tacl_a_00342
},
eprint = { 
        https://doi.org/10.1162/tacl_a_00342
}
}

@article{yoav-syntax,
  title={Assessing BERT's syntactic abilities},
  author={Goldberg, Yoav},
  journal={arXiv preprint arXiv:1901.05287},
  year={2019}
}

@inproceedings{dagan-rte,
  title={The PASCAL recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}

@inproceedings{structural-probe,
  author    = {John Hewitt and
               Christopher D. Manning},
  title     = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT}},
  pages     = {4129--4138},
  year      = {2019},
  url       = {https://aclweb.org/anthology/papers/N/N19/N19-1419/},
  timestamp = {Fri, 07 Jun 2019 14:36:16 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/HewittM19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Bender2020ClimbingTN,
  title={Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data},
  author={Emily M. Bender and Alexander Koller},
  booktitle={ACL},
  year={2020}
}

@inproceedings{zhou2020evaluating,
  title={Evaluating Commonsense in Pre-Trained Language Models.},
  author={Zhou, Xuhui and Zhang, Yue and Cui, Leyang and Huang, Dandan},
  booktitle={AAAI},
  pages={9733--9740},
  year={2020}
}

@inproceedings{berant2011global,
  title={Global learning of typed entailment rules},
  author={Berant, Jonathan and Dagan, Ido and Goldberger, Jacob},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages={610--619},
  year={2011}
}

@phdthesis{berant2012global,
  title={Global Learning of Textual Entailment Graphs},
  author={Berant, Jonathan},
  year={2012},
  school={Tel Aviv University}
}

@article{javad2018learning,
  title={Learning typed entailment graphs with global soft constraints},
  author={Javad Hosseini, Mohammad and Chambers, Nathanael and Reddy, Siva and Holt, Xavier R and Cohen, Shay B and Johnson, Mark and Steedman, Mark},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={703--717},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{hosseini2019duality,
  title={Duality of Link Prediction and Entailment Graph Induction},
  author={Hosseini, Mohammad Javad and Cohen, Shay B and Johnson, Mark and Steedman, Mark},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4736--4746},
  year={2019}
}

@InProceedings{mnli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@inproceedings{trex,
  title={T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples},
  author={Elsahar, Hady and Vougiouklis, Pavlos and Remaci, Arslen and Gravier, Christophe and Hare, Jonathon and Laforest, Frederique and Simperl, Elena},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@article{alpaqa,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{snli,
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Year = {2015}
}

@inproceedings{schmitt2019sherliic,
  title={SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference},
  author={Schmitt, Martin and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={902--914},
  year={2019}
}

@inproceedings{spike,
    title = "Syntactic Search by Example",
    author = "Shlain, Micah  and
      Taub-Tabib, Hillel  and
      Sadde, Shoval  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-demos.3",
    doi = "10.18653/v1/2020.acl-demos.3",
    pages = "17--23",
    abstract = "We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs. In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine. This allows for rapid exploration, development and refinement of syntax-based queries. We demonstrate the system using queries over two corpora: the English wikipedia, and a collection of English pubmed abstracts. A demo of the wikipedia system is available at https://allenai.github.io/spike/ .",
}

@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{mccoy2019right,
  title={Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
  author={McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3428--3448},
  year={2019}
}

@inproceedings{consistent-qa,
    title = "Logic-Guided Data Augmentation and Regularization for Consistent Question Answering",
    author = "Asai, Akari  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.499",
    doi = "10.18653/v1/2020.acl-main.499",
    pages = "5642--5650",
}

@article{andersen2001easy,
  title={Easy cases of probabilistic satisfiability},
  author={Andersen, Kim Allan and Pretolani, Daniele},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={33},
  number={1},
  pages={69--91},
  year={2001},
  publisher={Springer}
}

@incollection{hansen2000probabilistic,
  title={Probabilistic satisfiability},
  author={Hansen, Pierre and Jaumard, Brigitte},
  booktitle={Handbook of Defeasible Reasoning and Uncertainty Management Systems},
  pages={321--367},
  year={2000},
  publisher={Springer}
}

@inproceedings{Thimm:2009d,
	Author = {Matthias Thimm},
	Booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI'09)},
	Editor = {Jeff Bilmes and Andrew Ng},
	Month = {June},
	Pages = {530--537},
	Location = {Montreal, Canada},
	Publisher = {AUAI Press},
	Title = {{Measuring Inconsistency in Probabilistic Knowledge Bases}},
	Year = {2009}
}

@article{Thimm:2013,
	Author = {Matthias Thimm},
	Journal = {Artificial Intelligence},
	Month = {April},
	Pages = {1--24},
	Title = {Inconsistency Measures for Probabilistic Logics},
	Volume = {197},
	Year = {2013}
}

@article{muino2011measuring,
  title={Measuring and repairing inconsistency in probabilistic knowledge bases},
  author={Mui{\~n}o, David Picado},
  journal={International Journal of Approximate Reasoning},
  volume={52},
  number={6},
  pages={828--840},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{
petroni2020how,
title={How Context Affects Language Models' Factual Predictions},
author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rockt{\"a}schel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
booktitle={Automated Knowledge Base Construction},
year={2020},
url={https://openreview.net/forum?id=025X0zPfn}
}

@inproceedings{robertas,
    title = "Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)",
    author = "Warstadt, Alex  and
      Zhang, Yian  and
      Li, Xiaocheng  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.16",
    doi = "10.18653/v1/2020.emnlp-main.16",
    pages = "217--235",
}

@article{tsne,
  title={Visualizing data using t-{SNE}},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  pages={2579--2605},
  year={2008}
}

@inproceedings{ribeiro-etal-2019-red,
    title = "Are Red Roses Red? Evaluating Consistency of Question-Answering Models",
    author = "Ribeiro, Marco Tulio  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1621",
    doi = "10.18653/v1/P19-1621",
    pages = "6174--6184",
}

@inproceedings{vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@inproceedings{squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016}
}

@inproceedings{alberti2019synthetic,
  title={Synthetic QA Corpora Generation with Roundtrip Consistency},
  author={Alberti, Chris and Andor, Daniel and Pitler, Emily and Devlin, Jacob and Collins, Michael},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6168--6173},
  year={2019}
}

@inproceedings{du2019consistent,
  title={Be Consistent! Improving Procedural Text Comprehension using Label Consistency},
  author={Du, Xinya and Dalvi, Bhavana and Tandon, Niket and Bosselut, Antoine and Yih, Wen-tau and Clark, Peter and Cardie, Claire},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2347--2356},
  year={2019}
}

@inproceedings{kryscinski2020evaluating,
  title={Evaluating the factual consistency of abstractive text summarization},
  author={Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9332--9346},
  year={2020}
}

@inproceedings{ribeiro-etal-2020-beyond,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
}

@inproceedings{geva2020injecting,
  title={Injecting Numerical Reasoning Skills into Language Models},
  author={Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  booktitle={Association for Computational Linguistics (ACL)},
  year={2020},
}

@inproceedings{squad2,
  title={Know What You Donâ€™t Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={784--789},
  year={2018}
}

@inproceedings{Xiong2020Pretrained,
  author    = {Wenhan Xiong and
Jingfei Du and
William Yang Wang and
Veselin Stoyanov},
title     = {Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language
Model},
booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
publisher = {OpenReview.net},
year      = {2020},
url       = {https://openreview.net/forum?id=BJlzm64tDH},
timestamp = {Thu, 07 May 2020 17:11:47 +0200},
biburl    = {https://dblp.org/rec/conf/iclr/XiongDWS20.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{squad-paraphrase,
  title={Improving the robustness of question answering systems to question paraphrasing},
  author={Gan, Wee Chung and Ng, Hwee Tou},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6065--6075},
  year={2019}
}

@misc{amnesic_probing,
Author = {Yanai Elazar and Shauli Ravfogel and Alon Jacovi and Yoav Goldberg},
Title = {Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals},
Year = {2020},
Eprint = {arXiv:2006.00995},
}

@inproceedings{hewitt2019structural,
 author = {Hewitt, John and Manning, Christopher D.},
 booktitle = {North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
 location = {Minneapolis, USA},
 publisher = {Association for Computational Linguistics},
 title = {{A} Structural Probe for Finding Syntax in Word Representations},
 url = {https://nlp.stanford.edu/pubs/hewitt2019structural.pdf},
 year = {2019}
}

@inproceedings{tenney2019bert,
  title={BERT Rediscovers the Classical NLP Pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@inproceedings{zhang2020language,
  title={Do Language Embeddings capture Scales?},
  author={Zhang, Xikun and Ramachandran, Deepak and Tenney, Ian and Elazar, Yanai and Roth, Dan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={4889--4896},
  year={2020}
}

@article{denis2009global,
  title={Global joint models for coreference resolution and named entity classification},
  author={Denis, Pascal and Baldridge, Jason},
  journal={Procesamiento del Lenguaje Natural},
  volume={42},
  year={2009}
}

@inproceedings{chang2011inference,
  title={Inference protocols for coreference resolution},
  author={Chang, Kai-Wei and Samdani, Rajhans and Rozovskaya, Alla and Rizzolo, Nick and Sammons, Mark and Roth, Dan},
  booktitle={Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task},
  pages={40--44},
  year={2011}
}

@inproceedings{nguyen2016integrating,
  title={Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction},
  author={Nguyen, Kim Anh and im Walde, Sabine Schulte and Vu, Ngoc Thang},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={454--459},
  year={2016}
}

@article{what_is_paraphrase,
  title={What is a paraphrase?},
  author={Bhagat, Rahul and Hovy, Eduard},
  journal={Computational Linguistics},
  volume={39},
  number={3},
  pages={463--472},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{chi-etal-2020-finding,
    title = "Finding Universal Grammatical Relations in Multilingual {BERT}",
    author = "Chi, Ethan A.  and
      Hewitt, John  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.493",
    doi = "10.18653/v1/2020.acl-main.493",
    pages = "5564--5577",
}

@inproceedings{ravfogel-etal-2020-unsupervised,
    title = "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations",
    author = "Ravfogel, Shauli  and
      Elazar, Yanai  and
      Goldberger, Jacob  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.9",
    doi = "10.18653/v1/2020.blackboxnlp-1.9",
    pages = "91--106",
}

    @inproceedings{gonen-etal-2020-greek,
    title = "It{'}s not {G}reek to m{BERT}: Inducing Word-Level Translations from Multilingual {BERT}",
    author = "Gonen, Hila  and
      Ravfogel, Shauli  and
      Elazar, Yanai  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.5",
    doi = "10.18653/v1/2020.blackboxnlp-1.5",
    pages = "45--56",
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{marvin-linzen-2018-targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
}


@software{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@inproceedings{dror2018hitchhiker,
  title={The hitchhikerâ€™s guide to testing statistical significance in natural language processing},
  author={Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1383--1392},
  year={2018}
}

@article{dror2020statistical,
  title={Statistical Significance Testing for Natural Language Processing},
  author={Dror, Rotem and Peled-Cohen, Lotem and Shlomov, Segev and Reichart, Roi},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={13},
  number={2},
  pages={1--116},
  year={2020},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5185--5198},
  year={2020}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{dhingra2020differentiable,
    title={Differentiable Reasoning over a Virtual Knowledge Base},
    author={Dhingra, Bhuwan and Zaheer, Manzil and Balachandran, Vidhisha and Neubig, Graham and Salakhutdinov, Ruslan and Cohen, William W},
    booktitle={International Conference on Learning Representations},
    year={2019}
}

@article{thorne2020neural,
  title={Neural Databases},
  author={Thorne, James and Yazdani, Majid and Saeidi, Marzieh and Silvestri, Fabrizio and Riedel, Sebastian and Halevy, Alon},
  journal={arXiv preprint arXiv:2010.06973},
  year={2020}
}

@inproceedings{adversarial_attacks,
title	= {Intriguing properties of neural networks},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
year	= {2014},
URL	= {http://arxiv.org/abs/1312.6199},
booktitle	= {International Conference on Learning Representations}
}

@inproceedings{jia2017adversarial,
  title={Adversarial Examples for Evaluating Reading Comprehension Systems},
  author={Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={2021--2031},
  year={2017}
}

@inproceedings{ribeiro2018semantically,
  title={Semantically equivalent adversarial rules for debugging nlp models},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={856--865},
  year={2018}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@inproceedings{roberts2020much,
  title={How Much Knowledge Can You Pack into the Parameters of a Language Model?},
  author={Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={5418--5426},
  year={2020}
}

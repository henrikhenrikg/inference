\section{Improving Consistency in PLMs}
\label{sec:adding_consistency}

In the previous sections, we showed pretrained models are generally not consistent in their predictions, and previous works have noticed the lack of this property in a variety of downstream tasks.
% , and since PLMs are used for downstream tasks, lack of consistency is likely to affect them as well. 
% \nk{suggestion: cut: "lack of consistency is likely to affect them as well." , add: 
An ideal model would exhibit the consistency property after pretraining, and would then be able to transfer it to different downstream tasks. We therefore ask:
% Ideally, a consistent PLM would also reflect this property in downstream tasks.
Can we enhance current PLMs and make them more consistent?

\subsection{Consistency Improved PLMs}
% \nk{Maybe we need to emphasize already when introducing this that this is more of a case study rather than solving the issue}
We propose to improve the consistency of PLMs by continuing the pretraining step with a novel consistency loss. %\nk{Should we refer to the paper that does it in a similar way?}
We make use of the T-REx tuples and the paraphrases from \resource{}.

% \enote{hs}{below: why is the notation used here different from the rest of the paper?}
For each relation $r_i$, we
have a set of paraphrased patterns $P_i$ describing that relation.
%, each $P_j^i$ represents a paraphrase of some relation $R^i$.
We use a PLM to encode all patterns in $P_i$, after populating a subject that corresponds to the relation $r_i$ and a mask token. We expect the model to make the same prediction for the masked token for all patterns.
% Then we consider the probability distribution over the vocabulary of the masked word, denoted by $D^i_l$. Our goal is to make the distribution of every pair $D^i_l,D^i_m$ as close as possible.


\paragraph{Consistency Loss Function}
% Given two paraphrases from \resource{}, we expect the predictions of the masked tokens to be identical. However, as we showed in the previous sections, it is not the case. \nk{this is repeating the intro of section 8}
As we evaluate the model using acc@1, the straight-forward consistency loss would require these predictions to be identical:
\begin{gather*} 
\min_{\theta} \mbox{sim}(\arg\max_i f_\theta(P_n)[i], \arg\max_j f_\theta(P_m)[j])%\\
%s.t. Q_n = f(P_n; \theta), Q_m = f(P_m; \theta)
\end{gather*}
where $f_\theta(P_n)$ is the output of an encoding function (e.g., BERT) parameterized by $\theta$ (a vector) over input $P_n$, and $f_\theta(P_n)[i]$ is the score of the $i$th vocabulary item of the model.

% \enote{hs}{last bit: the score of the $i$th element of the
%   language models' input
%   vocabulary?}

%where $Q_n,Q_m$ are the vocabulary distributions for the masked token in the patterns $P_n,P_m$  and $f$ is the encoding function, e.g. BERT, parameterized by $\theta$.
% \nk{Is it necessary to talk about this option? Also we do that for the local training, right?}

However, this objective contains a comparison between the output of two argmax operations, making it discrete and discontinuous, and hard to optimize in a gradient-based framework. We instead relax the objective, and require that the predicted \emph{distributions} $Q_n = \mbox{softmax}(f_\theta(P_n))$, rather than the top-1 prediction, be identical to each other. %we use a softer constraint.
%Instead of optimizing for the same argmax, we optimize for the same distribution.
% We propose to enforce consistency, by requiring the distribution of the masked tokens to be identical. 
We use two-sided KL Divergence to measure similarity between distributions: $D_{KL}(Q_n^{r_i}||Q_m^{r_i}) + D_{KL}(Q_m^{r_i}||Q_n^{r_i})$
%the distributions $Q_n,Q_m$
%more similar. Since DKL is not a symmetric metric, we use a two-sided DKL for two patterns $P^i_m,P^i_n$:
%$D_{KL}(Q^i_n||Q^i_m) + D_{KL} (Q^i_m||Q^i_n)$
where $Q_n^{r_i}$ is the predicted distribution for pattern $P_n$ of relation $r_i$.

% \enote{hs}{above: is this simply the jensen shannon divdergence?}

As most of the vocabulary is not relevant for the
predictions, we filter it down to the $k$ tokens from the candidate set of each
relation (\S\ref{sec:framework}). We want to
maintain the original capabilities of the
model -- focusing on the candidate set helps to achieve this goal since most of the vocabulary is not affected by our new
loss.

% From preliminary results, we noticed that using only the candidate set is beneficial.

% We typically have multiple paraphrases; we use all of them, which can help in enforcing a more general solution. 
To encourage a more general solution, we make use of all the paraphrases together, and enforce all predictions to be as close as possible.
% Moreover, to achieve a more general solution, we use all paraphrases per relation, and optimize for a consistent 
Thus, the consistency loss for all pattern pairs for a particular relation $r^i$ is:
\[
\mathcal{L}_{c} = \sum_{n=1}^k \sum_{m=n+1}^k D_{KL}(Q^{r_i}_n||Q^{r_i}_m) + D_{KL}(Q^{r_i}_m||Q^{r_i}_n)
\]


\paragraph{MLM Loss}
Since the consistency loss is different from the
Cross-Entropy loss the PLM is trained on, we find it
important to continue the MLM loss on text data, similar to previous work \cite{geva2020injecting}.

We consider two alternatives for continuing the pretraining objective: (1) MLM on Wikipedia and (2) MLM on the patterns of the relations used for the consistency loss. We found that the latter works better. We denote this loss by $\mathcal{L}_{MLM}$.


\paragraph{Consistency Guided MLM Continual Training}

Combining our novel consistency loss with
the regular MLM loss, we continue the PLM training by
combining the two losses. The combination of the two losses
is determined by a hyperparameter $\lambda$, resulting in
the following final loss function:
\[
\mathcal{L} = \lambda \mathcal{L}_c + \mathcal{L}_{MLM}
\]
This loss is computed per relation, for one KB tuple. We have many of these instances, which we require to behave similarly. Therefore, we batch together $l=8$ tuples from the same relation and apply the consistency loss function to all of them.



% \paragraph{Relations used for training}
% The TREx relations contain different types and contain
% many location-related relations.

\subsection{Setup}


Since we evaluate our method on unseen relations, we also
split train and test by relation type (e.g., location-based relations, which are very common
in T-REx).  Moreover, our method is aimed to be simple,
effective, and to require only minimal supervision. Therefore,
we opt to use only three relations:
\textit{original-language}, \textit{named-after}, and
\textit{original-network}; these were chosen randomly, out of
the non-location related relations.\footnote{Many
  relations are location-based -- no training on them prevents train-test leakage.} 
For validation, we randomly pick three
relations of the remaining relations and use the remaining
twenty-five for testing.

We perform minimal tuning of the parameters \yenew{($\lambda \in {0.1, 0.5, 1}$)} to pick the best model, train for 3 epochs, and select the best model based on  \textit{Consistent-Acc} on the validation set.
For efficiency reasons, we use the base version of BERT.


% \nk{People would not understand why we exclude
% locations. I would cut this?}.

\subsection{Improved Consistency Results}

\input{tables/ft_consistency}

The results are presented in Table
\ref{tab:consistency-ft}. We report aggregated results
for the 25 relations in the test.
We again
report macro average (mean over relations) and
standard deviation.  We report the results of the majority
baseline (first row),   BERT-base  (second row)
and our new model (BERT-ft, third row).  First, we note
that our model significantly improves consistency:
to 64.0\%
(compared with 58.2\% for BERT-base,
an increase
of 5.8 points).  \textit{Accuracy} also improves compared to BERT-base, from 45.6\% to 47.4\%. Finally, and most
importantly, we see an increase of 5.9 points in
\textit{Consistent-Acc}, which is achieved due to the improved
consistency of the model.  Notably, these improvements
arise from training on merely three relations, meaning that
the model improved its consistency ability and generalized
to new relations.  We measure the statistical
significance of our method compared to the BERT baseline,
using McNemar's test (following
\citet{dror2018hitchhiker,dror2020statistical}) and find all
results to be significant ($p \ll 0.01$).

% \nk{I would emphasize here again that it was able to transfer consistency from the seen to the unseen relations and we should maybe add that this is not only syntax-based (is it?)}

We also perform an ablation study to quantify the utility of
the different components. First, we report on the finetuned
model without the consistency loss
(-consistency). Interestingly, it does improve over the
baseline (BERT-base), but it lags behind our finetuned model.
Second, applying our loss on the candidate set rather than
on the entire vocabulary is beneficial (-typed). Finally, by
not performing the MLM training on the generated patterns
(-MLM), the consistency results improve significantly
(80.8\%); however, this also hurts  \textit{Accuracy} and \textit{Consistent-Acc}.
MLM training seems to serve as a regularizer
that prevents catastrophic forgetting.


Our ultimate goal is to improve consistency in PLMs for better performance on downstream tasks. Therefore, we also experiment with finetuning on SQuAD \cite{squad}, and evaluating on paraphrased questions from SQuAD \cite{squad-paraphrase} using our consistency model. However, the results perform on par with the baseline model, both on SQuAD and the paraphrase questions. More research is required to show that consistent PLMs can also benefit downstream tasks.

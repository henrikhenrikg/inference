% \section{Consistency Probing of Knowledge}
\section{Probing PLMs for Consistency}
\label{sec:probe}

In this section, we formally define consistency and describe
our framework for probing the consistency of PLMs.

\subsection{Consistency}
We define a model as \emph{consistent} if, given  two
\textit{cloze-phrases} such as 
 ``Seinfeld originally aired on [MASK]'' and
``Seinfeld premiered on [MASK]'' that
are \textit{quasi-paraphrases} it makes the same
prediction.\footnote{A \textit{quasi-paraphrase} --
  introduced by \citet{what_is_paraphrase} -- is a more
  fuzzy version of a paraphrase. The concept does not rely
  on the strict, logical definition of paraphrase and
  allows to operationalize concrete uses of
  paraphrases. This definition is in the spirit of the RTE
  definition \cite{dagan-rte}, which similarly supports a
  more flexible use of the notion of entailment.}
\enote{hs}{I don't understand what this sentence says
-- in  addition to what was previously already said:
  
In the context of PLM knowledge consistency, if two cloze-phrases 
% \yg{we didn't define cloze-phrases yet. do they include a relation tuple?} 
of some relation (e.g., \textit{originally-aired-on}) are quasi-paraphrases, masked language models should predict the same entity for the masked item. 
% \sr{I changed the wording here. Is it still what you meant? a natural question now is ``why objects".}
% \yg{can it be "masked item"? or is it really a syntactic
% object?}
}
In the rest of the paper, we use the terms \textit{paraphrase} and \textit{quasi-paraphrase} interchangeably.

 
\enote{hs}{following paragraph: this sounds like quite a big
  problem to me: isn't it a huge assumption that the order
  of prediction will be the same for all paraphrases? is
  this seomthing we woudl expect of human performance?

  Can we move this to a later point in the paper and say:
  this is something that we will investigate in the future?}
We note that this definition does not take into account the type (e.g., one-to-one or many-to-many) of the relation. %\yg{what relation? we didn't say relation in this secction until now.} 
For instance, some relations are defined as \textit{many-to-many}, and as such, more than a single item can be factually correct, even for quasi-paraphrases. However, we still expect that the order will remain, and thus even if the alternative answer is factually correct, we still consider cases with different answers as a non-consistent result.

\subsection{The Framework}
\label{sec:framework}
Let
$D = \{D_1, D_2,
\dots, D_m\}$
be a set of sets of KB triples,
where each $D_i$ contains factual statements
that express a specific relation $R_i$ such as ``originally aired
on''. Each $D_i$ is composed of $n$ examples $D_i = \{d_1^i,
d_2^i, \dots, d_n^i\}$, where each $d_j^i = <s_j^i,o_j^i>$
is a subject-object tuple from the relation $R_i$. 
If $R_1=\text{``originally
  aired on"}$, then $<\text{\textit{Homeland}},
\text{\textit{Showtime}}>$
is an example of a tuple
tuple in $D_1$.
Moreover, every relation $R_i$ is associated with a set of cloze-patterns $P_i$ that are \textit{quasi-paraphrases} and express the same relation. For example, $P_1=\{p_1^1, p_2^1, \dots, p_n^1\}$ contain the patterns ``\subj{} was originally aired on \obj{}'' and ``\subj{} was premiered on \obj{}''.
\nk{I think we should have someone who is not familiar with our setup check this passage. I am not sure if everyone will understand it}
\yg{I agree, hard to follow. I propose to possible solutions: (1) start bottom-up rather than top-down: this is a pattern, here is a group of patterns, now we collect them into a set... (2) visualize it in a figure (instead of the text).}
\am{while I'm familiar with this work, reading this paragraph for the first time I can understand the definitions. I agree with Yoav that a visualization can help though}

Given some relation $R_i$, a subject-object tuple $d_j^i \in D_i$ (e.g., `Homeland', `Showtime') and two paraphrases $p_k^i, p_l^i \in P_i$ associated with this relation (such as ``\subj{} was originally aired on \obj{}'' and ``\subj{} premiered on \obj{}''), our goal is to test whether the model, consistently predicts the same object after filling the subject entity. To this end, we populate each of the subjects and a mask token to the pattern $p_k^i(s_j^i,mask)$, $p_l^i(s_j^i,mask)$
% we mask the objects $o_j^i$ 
and ask the model to predict them: ``Homeland was originally aired on [MASK]'' and ``Homeland was premiered on [MASK]''.  We expect a consistent model to predict the same entity. Notice that the consistency measure does not require the answers to be factually correct. While correctness is also an important property for KBs, we view it as separate objective and measure it independently. 

% If the model predicts the correct $o_j^i$ as the most probable completion, we take that as evidence for the storing of the factual knowledge $d_j^i$ in the model. 
% In this case, a model that can perform adequate inference over the syntactic and lexical alternations between $p_j^i$ and $p_k^i$ is expected to also correctly predict the object from the alternative patterns $p_k^i$.

\paragraph{Restricted Candidate Sets}
Ideally, we would focus on the top-1 prediction that the PLM produces in order to compare between paraphrases. However, since these models were not trained for the purpose of serving as KBs, but as LMs, words outside of the inspected entities are often possible (e.g., the object in ``\subj{} was originally aired on \obj{}'' can also be replaced with `tv', to make a proper English sentence).
Therefore, we restrict the vocabulary produced by the PLMs to a candidate set, that is the set of possible gold objects for each relation, as was also suggested in previous work \cite{Xiong2020Pretrained, nora@@}.

Note that this procedure is a relaxation of the general problem, especially in the context of KBs, however, low consistency results in this setup are even more alarming.
